project: synthetic-linear-laplace-dysweep
default_root_dir: experiments/sweep
count: 10000

# BASE configuration which is being used
base_config:
  seed_everything: 100
  trainer:
    # callbacks
    callbacks:
      - class_path: ocd.training.callbacks.data_visualizer.DataVisualizer
      - class_path: ocd.training.callbacks.phase_changer.PhaseChangerCallback
        init_args:
          monitor_validation: False
          monitor_training: True
          reset_optimizers: False
          reinitialize_weights_on_maximization: False
      - class_path: ocd.training.callbacks.save_results.SavePermutationResultsCallback
        init_args:
          num_samples: 5000
          log_every_n_epochs: 5
    accelerator: gpu
    devices: 1
    num_nodes: 0
    log_every_n_steps: 1
    check_val_every_n_epoch: 1
    enable_checkpointing: true
    enable_model_summary: true
    enable_progress_bar: false
  data:
    class_path: lightning_toolbox.DataModule
    init_args:
      batch_size: 128
      val_size: 0.01
      dataset: ocd.data.SyntheticOCDDataset
      dataset_args:
        observation_size: 1000
        standardization: True
        reject_outliers_n_far_from_mean: 5
        name: synthetic_parametric
        scm_generator: ocd.data.synthetic.ParametricSCMGenerator
        scm_generator_args:
          noise_type: laplace
          noise_parameters:
            loc: 0.0
            scale: 1.0
          s_function:
            # we perform normalization for same variance not to work
            function_descriptor: |
              def func(x):
                numpy.random.seed(int(numpy.mean(x)))
                return numpy.random.uniform() * numpy.ones_like(x)
            function_of_interest: func
          s_function_signature: steady
          t_function:
            # we perform normalization for varsort not to work
            function_descriptor: |
              def func(x):
                x[x > 100] = 100
                x[x < -100] = -100
                x_mean = numpy.mean(x)
                x_std = numpy.std(x)
                if x_std == 0:
                  x_std = 1
                x = (x - x_mean) / x_std
                return x
            function_of_interest: func
          t_function_signature: ident_normalize
          graph_generator: ocd.data.scm.GraphGenerator
          weight_s: [0.5, 1.5]
          weight_t: [0.5, 1.5]
  model:
    class_path: ocd.training.module.OCDafTrainingModule
    init_args:
      # the model
      model_cls: ocd.models.ocdaf.OCDAF
      model_args:
        # Permutation learner arguments
        use_permutation: True
        permutation_learner_cls: ocd.models.permutation.LearnablePermutation
        permutation_learner_args:
          gumbel_noise_std: >
            lambda self, training_module, **kwargs: 2 - (2 / (training_module.trainer.max_epochs)) * (training_module.current_epoch)
        # The flow model arguments
        layers: [10, 5, 5]
        populate_features: True
        layers_limit: [100, 300, 50]
        num_transforms: 1
        additive: False
        residual: False
        bias: true
        # activations
        activation: torch.nn.LeakyReLU
        activation_args:
          negative_slope: 0.1
        # scale transform
        scale_transform: true
        scale_transform_s_args:
          pre_act_scale: 0.4
          post_act_scale: 5.
        scale_transform_t_args:
          pre_act_scale: 0.02
          post_act_scale: 50.

      # the optimizer
      optimizer_parameters:
        - model.flow
        - model.permutation_model
      optimizer_is_active:
        - >
          lambda training_module: training_module.current_phase == 'maximization' if hasattr(training_module, 'current_phase') else True
        - >
          lambda training_module: training_module.current_phase == 'expectation' if hasattr(training_module, 'current_phase') else True
      grad_clip_val: 1.0
      lr: [0.01, 0.01]
      optimizer: [torch.optim.AdamW, torch.optim.AdamW]
      optimizer_args:
        - weight_decay: 0.1
        - weight_decay: 0.01
      scheduler:
        - torch.optim.lr_scheduler.ReduceLROnPlateau
        - torch.optim.lr_scheduler.ReduceLROnPlateau
      scheduler_args:
        - mode: min
          min_lr: 0.00005
          threshold: 0.001
        - mode: min
          min_lr: 0.0005
          threshold: 0.001
      scheduler_name: ["lr_scheduler_maximization", "lr_scheduler_expectation"]
      scheduler_optimizer: [0, 1]
      scheduler_monitor: ["loss", "loss"]
      objective_args:
        nll:
          code: >
            def func(training_module, batch):
              t = training_module.forward(batch)
              res = t['log_prob']
              return -res.mean()
          function_of_interest: func

sweep_configuration:
  method: grid
  metric:
    goal: minimize
    name: metrics/average-backward_relative_penalty
  parameters:
    dy__upsert:
      - sweep: True
        sweep_identifier: C_graph_size_and_scheduling
        sweep_alias:
          - n3
          - n4
          - n5
          - n6
        values:
          - data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_n3")
                  scm_generator_args:
                    graph_generator_args:
                      n: 3
            trainer:
              max_epochs: 1000
              # max_epochs: 1
              callbacks:
                dy__list__operations:
                  - dy__overwrite: 1
                init_args:
                  maximization_epoch_limit: 60
                  expectation_epoch_limit: 45
                  patience: 15
                  cooldown: 200
            model:
              init_args:
                model_args:
                  in_features: 3
                scheduler_args:
                  - factor: 0.5
                    patience: 55
                  - factor: 0.5
                    patience: 45
          - data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_n4")
                  scm_generator_args:
                    graph_generator_args:
                      n: 4
            trainer:
              max_epochs: 1500
              # max_epochs: 1
              callbacks:
                dy__list__operations:
                  - dy__overwrite: 1
                init_args:
                  maximization_epoch_limit: 80
                  expectation_epoch_limit: 55
                  patience: 15
                  cooldown: 200
            model:
              init_args:
                model_args:
                  in_features: 4
                scheduler_args:
                  - factor: 0.5
                    patience: 70
                  - factor: 0.5
                    patience: 55
          - data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_n5")
                  scm_generator_args:
                    graph_generator_args:
                      n: 5
            trainer:
              max_epochs: 1500
              # max_epochs: 1
              callbacks:
                dy__list__operations:
                  - dy__overwrite: 1
                init_args:
                  maximization_epoch_limit: 90
                  expectation_epoch_limit: 65
                  patience: 15
                  cooldown: 200
            model:
              init_args:
                model_args:
                  in_features: 5
                scheduler_args:
                  - factor: 0.5
                    patience: 100
                  - factor: 0.5
                    patience: 60
          - data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_n6")
                  scm_generator_args:
                    graph_generator_args:
                      n: 6
            trainer:
              max_epochs: 1500
              # max_epochs: 1
              callbacks:
                dy__list__operations:
                  - dy__overwrite: 1
                init_args:
                  maximization_epoch_limit: 90
                  expectation_epoch_limit: 65
                  patience: 15
                  cooldown: 200
            model:
              init_args:
                model_args:
                  in_features: 6
                scheduler_args:
                  - factor: 0.5
                    patience: 140
                  - factor: 0.5
                    patience: 110
      - sweep: True
        sweep_identifier: F_graph_type
        sweep_alias:
          - erdos
          - chain
          - full
        values:
          - data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_erdos")
                  scm_generator_args:
                    graph_generator_args:
                      graph_type: erdos_renyi
                      p: 0.4
          - data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_chain")
                  scm_generator_args:
                    graph_generator_args:
                      graph_type: chain
          - data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_full")
                  scm_generator_args:
                    graph_generator_args:
                      graph_type: full
      - sweep: True
        sweep_identifier: G_seed
        sweep_alias:
          - seedset0
          - seedset1
          - seedset2
          - seedset3
          - seedset4
        values:
          - seed_everything: 10
            data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_seed0")
                  seed: 11
                  scm_generator_args:
                    seed: 13
                    graph_generator_args:
                      seed: 14
          - seed_everything: 20
            data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_seed1")
                  seed: 21
                  scm_generator_args:
                    seed: 23
                    graph_generator_args:
                      seed: 24
          - seed_everything: 30
            data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_seed2")
                  seed: 21
                  scm_generator_args:
                    seed: 23
                    graph_generator_args:
                      seed: 24
          - seed_everything: 40
            data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_seed3")
                  seed: 41
                  scm_generator_args:
                    seed: 43
                    graph_generator_args:
                      seed: 44
          - seed_everything: 50
            data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_seed4")
                  seed: 51
                  scm_generator_args:
                    seed: 53
                    graph_generator_args:
                      seed: 54
      - sweep: True
        sweep_identifier: H_methods
        sweep_alias:
          - gumbel_top_k
          - straight_through_sinkhorn_with_standard
        values:
          - model:
              init_args:
                model_args:
                  permutation_learner_args:
                    permutation_type: gumbel-topk
          - model:
              init_args:
                model_args:
                  permutation_learner_args:
                    permutation_type: straight-through
            data:
              init_args:
                dataset_args:
                  standardization: True
                  reject_outliers_n_far_from_mean: 5
      - sweep: True
        sweep_identifier: B_additive_affine
        sweep_alias:
          - affine
          - additive
        values:
          - data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_affine")
                  scm_generator_args:
                    s_function: >
                      dy__eval(lambda x: x)
          - data:
              init_args:
                dataset_args:
                  name: >
                    dy__eval(lambda x: f"{x}_additive")
                  scm_generator_args:
                    s_function:
                      function_descriptor: |
                        def func(x):
                          return numpy.ones_like(x)
                      function_of_interest: func
                    s_function_signature: one
      - sweep: True
        sweep_identifier: Z_misspecification
        sweep_alias:
          - NoiseSpecified
          - NoiseMisspecified
        values:
          - model:
              init_args:
                model_args:
                  base_distribution: torch.distributions.laplace.Laplace
                  base_distribution_args:
                    loc: 0.0
                    scale: 1.0
          - model:
              init_args:
                model_args:
                  base_distribution: torch.distributions.normal.Normal
                  base_distribution_args:
                    loc: 0.0
                    scale: 1.0
