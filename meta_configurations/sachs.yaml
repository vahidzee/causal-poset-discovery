project: sachs-dysweep
default_root_dir: experiments/sweep
count: 10000

# BASE configuration which is being used
base_config:
  # lightning.pytorch==2.0.2
  seed_everything: true
  trainer:
    accelerator: gpu
    strategy: auto
    devices: 1
    num_nodes: 0
    precision: 32-true
    callbacks:
      - class_path: ocd.training.callbacks.data_visualizer.DataVisualizer
        init_args:
          image_size: null
          show_functions_in_dag: true
          show_original_statistics_on_histograms: true
      - class_path: ocd.training.callbacks.phase_changer.PhaseChangerCallback
        init_args:
          starting_phase: maximization
          monitor_validation: false
          monitor_training: true
          maximization_epoch_limit: 100
          expectation_epoch_limit: 80
          patience: 15
          threshold: 0.0001
          cooldown: 200
          reset_optimizers: false
          reinitialize_weights_on_maximization: false
          log_onto_logger: true
      - class_path: ocd.training.callbacks.save_results.SavePermutationResultsCallback
        init_args:
          save_every_n_epochs: null
          log_every_n_epochs: 5
          num_samples: 5000
          log_into_logger: true
          evaluation_metrics: null
          ignore_evaluation_metrics:
            - pc-shd
    fast_dev_run: false
    min_epochs: null
    max_steps: -1
    min_steps: null
    max_time: null
    limit_train_batches: null
    limit_val_batches: null
    limit_test_batches: null
    limit_predict_batches: null
    overfit_batches: 0.0
    val_check_interval: null
    check_val_every_n_epoch: 1
    num_sanity_val_steps: null
    log_every_n_steps: 1
    enable_checkpointing: true
    enable_progress_bar: false
    enable_model_summary: true
    accumulate_grad_batches: 1
    gradient_clip_val: null
    gradient_clip_algorithm: null
    deterministic: null
    benchmark: null
    inference_mode: true
    use_distributed_sampler: true
    profiler: null
    detect_anomaly: false
    barebones: false
    plugins: null
    sync_batchnorm: false
    reload_dataloaders_every_n_epochs: 0
    default_root_dir: null
  data:
    class_path: lightning_toolbox.DataModule
    init_args:
      dataset: ocd.data.real_world.sachs.SachsOCDDataset
      dataset_args:
        name: sachs_standardized_linear_short
        standardization: true
        reject_outliers_n_far_from_mean: 5
      train_dataset: null
      train_dataset_args: null
      val_size: 0.01
      val_dataset: null
      val_dataset_args: null
      test_dataset: null
      test_dataset_args: null
      transforms: null
      train_transforms: null
      val_transforms: null
      test_transforms: null
      batch_size: 128
      train_batch_size: null
      val_batch_size: null
      test_batch_size: null
      pin_memory: true
      train_pin_memory: null
      val_pin_memory: null
      test_pin_memory: null
      train_shuffle: true
      val_shuffle: false
      test_shuffle: false
      num_workers: 0
      train_num_workers: null
      val_num_workers: null
      test_num_workers: null
      seed: 0
      save_hparams: true
      initialize_superclass: true
  model:
    class_path: ocd.training.module.OCDafTrainingModule
    init_args:
      maximization_specifics: null
      expectation_specifics: null
      grad_clip_val: 1.0
      phases:
        - maximization
        - expectation
      model: null
      model_cls: ocd.models.ocdaf.OCDAF
      model_args:
        use_permutation: true
        permutation_learner_cls: ocd.models.permutation.LearnablePermutation
        permutation_learner_args:
          permutation_type: gumbel-topk
        base_distribution: torch.distributions.Normal
        base_distribution_args:
          loc: 0.0
          scale: 1.0
        layers:
          - 30
          - 10
          - 10
        populate_features: true
        layers_limit:
          - 300
          - 100
          - 50
        num_transforms: 1
        additive: false
        residual: false
        bias: true
        scale_transform: true
        scale_transform_s_args:
          pre_act_scale: 0.1
          post_act_scale: 10.0
        scale_transform_t_args:
          pre_act_scale: 0.01
          post_act_scale: 100.0
        activation: torch.nn.LeakyReLU
        activation_args:
          negative_slope: 0.1
        in_features: 11
      objective: null
      objective_cls: lightning_toolbox.Objective
      objective_args:
        nll:
          code:
            "def func(training_module, batch):\n  t = training_module.forward(batch)\n\
            \  res = t['log_prob']\n  return -res.mean()\n"
          function_of_interest: func
      optimizer:
        - torch.optim.AdamW
        - torch.optim.AdamW
      optimizer_frequency: null
      optimizer_is_active:
        - "lambda training_module: training_module.current_phase == 'maximization' if
          hasattr(training_module, 'current_phase') else True

          "
        - "lambda training_module: training_module.current_phase == 'expectation' if
          hasattr(training_module, 'current_phase') else True

          "
      optimizer_parameters:
        - model.flow
        - model.permutation_model
      optimizer_args:
        - weight_decay: 0.1
        - weight_decay: 0.01
      lr:
        - 0.01
        - 0.01
      scheduler:
        - torch.optim.lr_scheduler.ReduceLROnPlateau
        - torch.optim.lr_scheduler.ReduceLROnPlateau
      scheduler_name:
        - lr_scheduler_maximization
        - lr_scheduler_expectation
      scheduler_optimizer:
        - 0
        - 1
      scheduler_args:
        - mode: min
          min_lr: 0.0001
          threshold: 0.0001
        - mode: min
          min_lr: 0.0001
          threshold: 0.0001
      scheduler_interval: epoch
      scheduler_frequency: 1
      scheduler_monitor:
        - loss
        - loss
      scheduler_strict: null
      save_hparams: true
      initialize_superclass: true
sweep_configuration:
  method: grid
  metric:
    goal: minimize
    name: metrics/best-backward_relative_penalty
  parameters:
    dy__upsert:
      - sweep: True
        sweep_identifier: B_seed
        sweep_alias:
          - seed100
          - seed200
          - seed300
          - seed400
          - seed500
        values:
          - seed_everything: 100
          - seed_everything: 200
          - seed_everything: 300
          - seed_everything: 400
          - seed_everything: 500
      - sweep: True
        sweep_identifier: A_scheduling
        sweep_alias:
          - fast1500
          - slow3000
        values:
          - trainer:
              max_epochs: 1500
              callbacks:
                dy__list__operations:
                  - dy__overwrite: 1
                init_args:
                  maximization_epoch_limit: 100
                  expectation_epoch_limit: 80
                  patience: 15
                  cooldown: 200
            model:
              init_args:
                scheduler_args:
                  - factor: 0.5
                    patience: 200
                  - factor: 0.5
                    patience: 180
          - trainer:
              max_epochs: 3000
              callbacks:
                dy__list__operations:
                  - dy__overwrite: 1
                init_args:
                  maximization_epoch_limit: 120
                  expectation_epoch_limit: 100
                  patience: 15
                  cooldown: 200
            model:
              init_args:
                scheduler_args:
                  - factor: 0.5
                    patience: 250
                  - factor: 0.5
                    patience: 210
      - sweep: True
        sweep_identifier: C_gumbel_scheduling
        sweep_alias:
          - Linear
          - FastFirst
          - FastLast
        values:
          - model:
              init_args:
                model_args:
                  permutation_learner_args:
                    gumbel_noise_std:
                      code: |
                        def func1(self, training_module, **kwargs):
                          
                          callback = training_module.trainer.callbacks[1]
                          a = callback.maximization_epoch_limit
                          b = callback.expectation_epoch_limit
                          tot_epochs_on_exp = (training_module.trainer.max_epochs // (a + b)) * b
                          rem = training_module.trainer.max_epochs - (training_module.trainer.max_epochs // (a + b)) * (a + b)
                          if callback.starting_phase == 'maximization':
                            tot_epochs_on_exp += max(0, rem - a)
                          else:
                            tot_epochs_on_exp += min(rem, b)
                            
                          if not hasattr(training_module, 'last_std'):
                            training_module.last_std = 0 
                          # if you don't have the epoch from before then assign it
                          flag = False
                          if not hasattr(training_module, 'last_epoch_std_sched'):
                            flag = True
                            training_module.last_epoch_std_sched = training_module.current_epoch

                          if training_module.current_phase != 'maximization' and (flag or training_module.last_epoch_std_sched != training_module.current_epoch):
                            training_module.last_std += 1
                          training_module.last_epoch_std_sched = training_module.current_epoch

                          x = max(min(training_module.last_std / tot_epochs_on_exp, 1.0), 0.0)            
                          return 2.0 - 2.0 * x

                      function_of_interest: func1
          - model:
              init_args:
                model_args:
                  permutation_learner_args:
                    gumbel_noise_std:
                      code: |
                        def func2(self, training_module, **kwargs):
                          
                          callback = training_module.trainer.callbacks[1]
                          a = callback.maximization_epoch_limit
                          b = callback.expectation_epoch_limit
                          tot_epochs_on_exp = (training_module.trainer.max_epochs // (a + b)) * b
                          rem = training_module.trainer.max_epochs - (training_module.trainer.max_epochs // (a + b)) * (a + b)
                          if callback.starting_phase == 'maximization':
                            tot_epochs_on_exp += max(0, rem - a)
                          else:
                            tot_epochs_on_exp += min(rem, b)
                            
                          if not hasattr(training_module, 'last_std'):
                            training_module.last_std = 0 
                          # if you don't have the epoch from before then assign it
                          flag = False
                          if not hasattr(training_module, 'last_epoch_std_sched'):
                            flag = True
                            training_module.last_epoch_std_sched = training_module.current_epoch

                          if training_module.current_phase != 'maximization' and (flag or training_module.last_epoch_std_sched != training_module.current_epoch):
                            training_module.last_std += 1
                          training_module.last_epoch_std_sched = training_module.current_epoch

                          x = max(min(training_module.last_std / tot_epochs_on_exp, 1.0), 0.0)            
                          return 2.0 * (1 - x) ** 3

                      function_of_interest: func2
          - model:
              init_args:
                model_args:
                  permutation_learner_args:
                    gumbel_noise_std:
                      code: |
                        def func3(self, training_module, **kwargs):
                          
                          callback = training_module.trainer.callbacks[1]
                          a = callback.maximization_epoch_limit
                          b = callback.expectation_epoch_limit
                          tot_epochs_on_exp = (training_module.trainer.max_epochs // (a + b)) * b
                          rem = training_module.trainer.max_epochs - (training_module.trainer.max_epochs // (a + b)) * (a + b)
                          if callback.starting_phase == 'maximization':
                            tot_epochs_on_exp += max(0, rem - a)
                          else:
                            tot_epochs_on_exp += min(rem, b)
                            
                          if not hasattr(training_module, 'last_std'):
                            training_module.last_std = 0 
                          # if you don't have the epoch from before then assign it
                          flag = False
                          if not hasattr(training_module, 'last_epoch_std_sched'):
                            flag = True
                            training_module.last_epoch_std_sched = training_module.current_epoch

                          if training_module.current_phase != 'maximization' and (flag or training_module.last_epoch_std_sched != training_module.current_epoch):
                            training_module.last_std += 1
                          training_module.last_epoch_std_sched = training_module.current_epoch

                          x = max(min(training_module.last_std / tot_epochs_on_exp, 1.0), 0.0)            
                          return 2.0 * (1 - x**3)

                      function_of_interest: func3

      - trainer:
          callbacks:
            dy__list__operations:
              - dy__overwrite: 2
            init_args:
              save_path:
                dy__eval:
                  expression: |
                    def func(conf):
                      return f"experiments/saves/sachs/seed-{conf['seed_everything']}-maxepoch-{conf['trainer']['max_epochs']}-sched-{conf['model']['init_args']['model_args']['permutation_learner_args']['gumbel_noise_std']['function_of_interest']}"
                  function_of_interest: func
              save_every_n_epochs: 500
