seed_everything: true
trainer:
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: simple-trainer
  accelerator: gpu
  strategy: auto
  devices: 1
  num_nodes: 0
  precision: 32-true
  callbacks:
    - class_path: ocd.training.callbacks.data_visualizer.DataVisualizer
      init_args:
        image_size: null
        show_functions_in_dag: true
        show_original_statistics_on_histograms: true
    - class_path: ocd.training.callbacks.phase_changer.PhaseChangerCallback
      init_args:
        starting_phase: maximization
        monitor_validation: false
        monitor_training: true
        maximization_epoch_limit: 100
        expectation_epoch_limit: 80
        patience: 15
        threshold: 0.0001
        cooldown: 200
        reset_optimizers: false
        reinitialize_weights_on_maximization: false
        log_onto_logger: true
    - class_path: ocd.training.callbacks.save_results.SavePermutationResultsCallback
      init_args:
        save_every_n_epochs: null
        log_every_n_epochs: 5
        num_samples: 5000
        log_into_logger: true
        evaluation_metrics: null
        ignore_evaluation_metrics:
          - pc-shd
  fast_dev_run: false
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: 1
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null
data:
  class_path: lightning_toolbox.DataModule
  init_args:
    dataset: ocd.data.real_world.sachs.SachsOCDDataset
    dataset_args:
      name: sachs
      standardization: true
      reject_outliers_n_far_from_mean: 5
    train_dataset: null
    train_dataset_args: null
    val_size: 0.01
    val_dataset: null
    val_dataset_args: null
    test_dataset: null
    test_dataset_args: null
    transforms: null
    train_transforms: null
    val_transforms: null
    test_transforms: null
    batch_size: 128
    train_batch_size: null
    val_batch_size: null
    test_batch_size: null
    pin_memory: true
    train_pin_memory: null
    val_pin_memory: null
    test_pin_memory: null
    train_shuffle: true
    val_shuffle: false
    test_shuffle: false
    num_workers: 0
    train_num_workers: null
    val_num_workers: null
    test_num_workers: null
    seed: 0
    save_hparams: true
    initialize_superclass: true
model:
  class_path: ocd.training.module.OCDafTrainingModule
  init_args:
    maximization_specifics: null
    expectation_specifics: null
    grad_clip_val: 1.0
    phases:
      - maximization
      - expectation
    model: null
    model_cls: ocd.models.ocdaf.OCDAF
    model_args:
      use_permutation: true
      permutation_learner_cls: ocd.models.permutation.LearnablePermutation
      permutation_learner_args:
        permutation_type: gumbel-topk
      base_distribution: torch.distributions.Normal
      base_distribution_args:
        loc: 0.0
        scale: 1.0
      layers:
        - 30
        - 10
        - 10
      populate_features: true
      layers_limit:
        - 300
        - 100
        - 50
      num_transforms: 1
      additive: false
      residual: false
      bias: true
      scale_transform: true
      scale_transform_s_args:
        pre_act_scale: 0.1
        post_act_scale: 10.0
      scale_transform_t_args:
        pre_act_scale: 0.01
        post_act_scale: 100.0
      activation: torch.nn.LeakyReLU
      activation_args:
        negative_slope: 0.1
      in_features: 11
    objective: null
    objective_cls: lightning_toolbox.Objective
    objective_args:
      nll:
        code:
          "def func(training_module, batch):\n  t = training_module.forward(batch)\n\
          \  res = t['log_prob']\n  return -res.mean()\n"
        function_of_interest: func
    optimizer:
      - torch.optim.AdamW
      - torch.optim.AdamW
    optimizer_frequency: null
    optimizer_is_active:
      - "lambda training_module: training_module.current_phase == 'maximization' if
        hasattr(training_module, 'current_phase') else True

        "
      - "lambda training_module: training_module.current_phase == 'expectation' if
        hasattr(training_module, 'current_phase') else True

        "
    optimizer_parameters:
      - model.flow
      - model.permutation_model
    optimizer_args:
      - weight_decay: 0.1
      - weight_decay: 0.01
    lr:
      - 0.01
      - 0.01
    scheduler:
      - torch.optim.lr_scheduler.ReduceLROnPlateau
      - torch.optim.lr_scheduler.ReduceLROnPlateau
    scheduler_name:
      - lr_scheduler_maximization
      - lr_scheduler_expectation
    scheduler_optimizer:
      - 0
      - 1
    scheduler_args:
      - mode: min
        min_lr: 0.0001
        threshold: 0.0001
      - mode: min
        min_lr: 0.0001
        threshold: 0.0001
    scheduler_interval: epoch
    scheduler_frequency: 1
    scheduler_monitor:
      - loss
      - loss
    scheduler_strict: null
    save_hparams: true
    initialize_superclass: true